---
title: "001-Bioinformatics_pipeline"
author: "Rebecca Cheek"
date: "8/14/2020"
output: html_document
---

Downloading the data 
```{sh}

#created "shrimp" folder in "scratch_dir"
#created "aws" conda environment and installed aws client as suggested by genohub for downloading the data
#access this conda environment by typing:
conda activate aws
#followed the genohub instructions for configuring my aws

aws configure

#created new "data_raw" directory to download raw data into
#downloaded data with:

aws s3 sync s3://genohub7542960 ./

#Downloaded the sample key excel file from genohub: "SampleKey-20030-01"
#created a "key.txt" file in the directory with the data that had genohub names and our ID names separated by a tab on each linecdn
#renamed the fastq files to start with our names and retain R1 or R2 info using script rename.sh
#all fastq files now look like: "Pop_ID_L001_R1/2_001.fastq.gz" where "Pop" and "ID" are our names


##RGCHeek update 15AUG2020

#re-downloaded the raw files from genohub into raw_data folder after messing up file names for populations and replicated the rename.sh

#had to create a new anaconda environment and activate it to get aws going, but the configuration was still active, so skipped a step there 



```

**Note** I've denoted the command prompt as '$' in any commands outside the sh code blocks in this RMarkdown

Navigate to the mysis folder using cd 

```{sh}

cd scratch_dir/shrimp/raw_data/20030-01


```


You'll see 336 files for Read 1 and Read two of each individual. They all must be named in a way that is recognizable by populations, so need to rename them all using the rename_populations.sh script. 

```{sh}
#!/bin/bash

for i in *.fastq.gz ;
do
mv $i $(echo $i | sed '
  s/_L001_/./g;
  s/R2_001/2/g;
   s/R1_001/1/g;
' )

done

```


Now that the files are in a naming format usable by populations, we can run the clone-filter program from stacks to identify and remove PCR-clones which are common in RADseq datasets. 

script 00-clone_filter to run as a batch job. This uses the random oligo embedded in the fasta header to identify clones. 
```{sh}

#!/bin/bash

#SBATCH --job-name=clone_filter
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/raw_data/clone_filtered.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/raw_data/clone_filtered.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com
#SBATCH --mail-type=END
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=10G

OUT=/home/ghalambor/scratch_dir/shrimp/raw_data/clone_filtered/
src=/home/ghalambor/scratch_dir/shrimp/raw_data/20030-01
FILES="
A_10
A_11
A_1
A_12
A_13
A_14
A_15
A_16
A_17
A_18
A_19
A_20
A_2
A_3
A_4
A_5
A_6
A_7
A_8
A_9
C_10
C_11
C_1
C_12
C_13
C_14
C_15
C_16
C_17
C_18
C_19
C_20
C_2
C_3
C_4
C_5
C_6
C_7
C_8
C_9
CLER_1
CLER_12
CLER_13
CLER_14
CLER_15
CLER_16
CLER_17
CLER_18
CLER_19
CLER_20
CLER_21
CLER_2
CLER_22
CLER_23
CLER_24
CLER_25
CLER_3
CLER_4
CLER_5
CLER_6
CLER_7
CLER_8
CLER_9
D_10
D_11
D_1
D_12
D_13
D_14
D_15
D_16
D_17
D_18
D_19
D_20
D_2
D_3
D_4
D_5
D_6
D_7
D_8
D_9
GGRO_10
GGRO_11
GGRO_1
GGRO_13
GGRO_14
GGRO_15
GGRO_16
GGRO_17
GGRO_19
GGRO_21
GGRO_2
GGRO_3
GGRO_4
GGRO_8
GGRO_9
J_10
J_11
J_1
J_12
J_13
J_14
J_15
J_16
J_17
J_18
J_19
J_20
J_2
J_3
J_4
J_5
J_6
J_7
J_8
J_9
LOWT_10
LOWT_1
LOWT_12
LOWT_13
LOWT_15
LOWT_18
LOWT_20
LOWT_2
LOWT_29
LOWT_3
LOWT_4
LOWT_5
LOWT_6
LOWT_7
LOWT_8
LOWT_9
R_10
R_11
R_1
R_12
R_13
R_14
R_15
R_16
R_17
R_18
R_19
R_20
R_2
R_3
R_4
R_5
R_6
R_7
R_8
R_9
RGRO_25
RGRO_26
RGRO_27
RGRO_28
RGRO_29
RGRO_31
RGRO_32
RGRO_33
RGRO_34
RGRO_35
RGRO_36
RGRO_37
RGRO_38
RGRO_39"

for sample in $FILES
do

clone_filter -1 $src/$sample.1.fastq.gz  -2 $src/$sample.2.fastq.gz  -i gzfastq -o $OUT --null_index --oligo_len_2 16

done


```


An example output is:$ 4930746 pairs of reads input. 2079775 pairs of reads output, discarded 0 pairs of reads, 57.82% clone reads.

So we loose a lot of data. Greater than 50% seems a little high since others tend to loose mroe around 30%, but its best to reduce the number of clones. This could be a sign of a lot of homogeneity in the shrimp.  

We can also improve denovo mapping by removing the first 10 degenerate base pairs using cutadapt.


Using Cutadapt v.1.16 (Martin 2011), we removed the 10 bp degenerative barcode sequences using a hard base pair removal option -u. Run in the clone filtered directory  

```{sh}

#!/bin/bash

#SBATCH --job-name=cut_adapt
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/raw_data/cut_reads/cut_adapt.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/raw_data/cut_reads/cut_adapt.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com

OUT=/home/ghalambor/scratch_dir/shrimp/raw_data/cut_reads

for sample in `ls *.fq.gz`    #List all fq.gz files in the folder and for each of these file run through the for loop

cutadapt -u 10 -o $OUT/$sample $sample

done



```


Add popmap for shrimp. This popmap consists of three, tab separated columns for the individual ID, Lake where the individual was sampled, as well as the phenotype "regular", and "jumbo". A vast majority of our individuals are regular morphs apart from Gross Lake which has the "jumbo" phenotype. 

What the basis of this "jumbo" phenotype is is unclear. It could be a polyploidy, which I'm not seeing in the read depth output from the non-clone filtered data, so unlikely. 
There is also the possibility that these are overwintered females which I'm not sure if it's possible to detect without a reference genome for sex chromosomes and no other phenotype information besides size. 

```{sh}

#note, linux can't read the line breaks in windows text files so use this command to get rid of the line breaks in linux
tr -d '\r' < mysis_popmap_pheno.txt > mysis_popmap_pheno

```


We do not have a reference genome for mysis shrimp, so we will be using the denovo_map pipeline from Stacks to align our reads. FYI,because the samples are paired end data it is usually good practice to specify --rm-pcr-duplicates in the denovo pipeline; however, samples were sequenced using ddRAD each sample is the same length and pcr-duplicates is intended for single digest sequencing with a random sheared end. So it will remove all but one of the read copies. So we used clone filter instead to remove our duplicate reads.  

Specify the populations -X parameter to get an idea of how many loci there should be with each denovo test.   

make two directories in the mysis_shrimp directory 
$ mkdir ./tests_denovo
$ mkdir ./tests_denovo/M3n2

Script 01_denovo_test.sh contains the script to submit the denovo map as a job. Using the command from the shrimp directory:

$ sbatch ./scripts/01_denovo_test.sh

```{sh}

#!/bin/bash

#SBATCH --job-name=mysis_denovo
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/tests_denovo/M3n2/test_denovo.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/tests_denovo/M3n2/test_denovo.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com
#SBATCH --mail-type=END


OUT=/home/ghalambor/scratch_dir/shrimp/tests_denovo/M3n2
POPMAP=/home/ghalambor/scratch_dir/shrimp/raw_data/mysis_popmap
FILEDIR=/home/ghalambor/scratch_dir/shrimp/raw_data/20030-01/

denovo_map.pl -T 8 -M 3 -n 2 -o $OUT --samples $FILEDIR --popmap $POPMAP --paired -X "ustacks:--max_locus_stacks 4" "popualtions: -r .80"


```

Will run the pipeline multiple times to optimize the different parameters including:
-M the number of mismatches allowed between loci when proc single individual (default 2)
-n Number of mismatches allowed between stacks (putative loci) during construction of the catalog (default 1)

The parameters:
-m Minimum depth of coverage required to create a stack (default 3) test @ 4 and 5 
--max_locus_stacks — maximum number of stacks at a single de novo locus (default 3) test at 4 & 5  

will also be tested but have to be run using and additional options -X "ustacks: -m 4" ect
Results of the tests are stored in the stacks_tests excel file. 


In order to get a sense of how changing the different parameters affects the number of loci, Paris et al. (2017) tested M0-8 and n parameters of the M value -1, =M, and +1. We can then alter the value or r in populations to see how the different tests affect the r80 of the sample (loci found in at least 80% of individuals). The goal is to maximize the number of polymorphic loci found in at least 80% of individuals. So, will need to run popualtions on all the tests to see how the -r .80 (Percentage of individuals that must possess a particular locus for it to be included in calculation of population-level statistics)


```{sh}


#run this in the different denovo test folders 
populations -P ./ -M /home/ghalambor/scratch_dir/shrimp/raw_data/mysis_popmap -r 0.80 -t 8


```



According to the results fo the Stacks test. It appears as though the shrimp are fairly homogeneous as the number of novel SNPs detected in 80% of the sampled individuals declines with greater M values (the number of mismatches allowed between loci when processing a single individual).At M1, the majority of polymorphism and SNPs were already captured, and the amount of polymorphic loci was relatively uniform with the highest polymorphism detected across 80% of the population was at M2. the n setting (Number of mismatches allowed between stacks (putative loci) during construction of the catalog) for M + 1 showed the greater number of snps. Therefore, it is recommended the settings for the denovo mapping is m3M2n3 according to the r80 rule dictated by (Paris et al. 2017).   


re-ran the denovo map with above parameters and got this output from gstacks 

Attempted to assemble and align paired-end reads for 952066 loci:
  0 loci had no or almost no paired-end reads (0.0%);
  71908 loci had paired-end reads that couldn't be assembled into a contig (7.6%);
  For the remaining 880158 loci (92.4%), a paired-end contig was assembled;
    Average contig size was 250.7 bp;
  439856 paired-end contigs overlapped the forward region (50.0%)
    Mean overlap: 27.6bp; mean size of overlapped loci after merging: 230.4;
  Out of 158922540 paired-end reads in these loci (mean 175.0 reads per locus),
    154053331 were successfuly aligned (96.9%);
  Mean insert length was 242.8, stdev: 50.6 (based on aligned reads in overlapped loci).

Genotyped 879855 loci:
  effective per-sample coverage: mean=10.2x, stdev=0.5x, min=9.3x, max=12.2x
  mean number of sites per locus: 243.5
  a consistent phasing was found for 1090922 of out 1714985 (63.6%) diploid loci needing phasing




*Filtering*

First, we will do a light filtering in populations from STACKS and export the file formats we need
-R,--min-samples-overall [float] — minimum percentage of individuals across populations required to process a locus
-p,--min-populations [int] — minimum number of populations a locus must be present in to process a locus.
--min-mac [int] — specify a minimum minor allele count required to process a SNP

```{sh}
#!/bin/bash

#SBATCH --job-name=populations
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/denovo_map/pre_filter/populations.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/denovo_map/pre_filter/populations.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com
#SBATCH --mail-type=END


OUT=/home/ghalambor/scratch_dir/shrimp/denovo_map/pre_filter
POPMAP=/home/ghalambor/scratch_dir/shrimp/raw_data/mysis_popmap_pheno
FILEDIR=/home/ghalambor/scratch_dir/shrimp/denovo_map

populations -P $FILDIR -O $OUT -M $POPMAP -p 1 -R 0.3 --min_mac 2 --vcf 


```

Also, convert the vcf to a 012 format using the vcftools package in anaconda 

```{sh}
#first activate the conda environment 

conda activate

#then convert the vcf to 012 file format from the denovo_map/pre-filter directory

vcftools --vcf populations.snps.vcf --012 --out mysis_filter_test

```



**WHOA**
  Option to check how the unfiltered VCF looks 


```{r}
library(tidyverse)
library(readr)
library(whoa)
library(vcfR)


v <- read.vcfR("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/pre_filter/mysis_pre_filter.vcf")

gfreqs <- exp_and_obs_geno_freqs(v)


geno_freqs_scatter <- function(gfc, alpha = 0.2, max_plot_loci = 500) {
  
  snps <- unique(gfc$snp)
  if(length(snps) > max_plot_loci) {
    ss <- sample(x = snps, size = max_plot_loci, replace = FALSE)
  } else {
    ss <- snps
  }
  g <- ggplot2::ggplot(gfc %>% dplyr::filter(snp %in% ss) , ggplot2::aes(x = p_exp, y = p_obs, colour = z_score)) +
    ggplot2::geom_jitter(alpha = alpha, position = ggplot2::position_jitter(width = 0.01, height = 0.01)) +
    ggplot2::facet_wrap(~ geno, nrow = 1) +
    ggplot2::geom_polygon(data = geno_freq_boundaries(), fill = NA, linetype = "dashed", colour = "black") +
    ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "solid") +
    scale_color_viridis_c()
  
  g
  
}


geno_freqs_scatter(gfreqs, alpha = 0.04, max_plot_loci = 1e15)  

#looks like ther are some miscalled heterozygotes 
# If we want to estimate the het miscall rate (over all read depth bins) we just set the minimum bin size to a very large value so it make just one bin:
overall <- infer_m(v, minBin = 1e15)
#> Preparing data structures for MCMC
#> Running MCMC
#> Tidying output

overall$m_posteriors


#  mean         lo95         hi95          total_n     mean_dp
# 0.01263127	  0.0123508  	 0.01292179	   15895181	   8.894416
#pretty good for Het miscall rate



```
  

Overall there is little evidence of high rates of heterozygote misscalls, but there are still some problamatic loci that need to be filtered out. 

*Preliminary check with GenoscapeRTools* 

Genoscape R tools is a series of utilities created by Eric Anderson which help investigate the amount of missing data in the SNP matrix to visualize where a good cutoff might be for filtering to maximize SNPs and individuals in dataset.  https://github.com/eriqande/genoscapeRtools

Prior to filtering, Our sampling design is:
Number of ind:
Carter = 20
Clear_Water = 23
Dillon = 20
Grand = 20
GGross = 15
RGross = 14
Jefferson = 20
Lower_Twin = 16
Ruedi = 20

```{r}
#Notice that the actual "012" file itself has been gzipped to save space.
#The latest version of this package allows that (so long as you are on a system like Unix...)

#First we gotta load the libraries we will need:
  
library(genoscapeRtools)
library(readr)
library(SNPRelate)


#Reading data in
#There's a function for that

mysis <- read_012(prefix = "C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/pre_filter/mysis_filter_test", gz = FALSE)
 
#Read 374552 items
#Read 168 items


#doing missing data calcs
#We can do this in a locus centric or an individual centric manner. Here we first do it in an indiv-centric way:

indv <- genoscapeRtools::miss_curves_indv(mysis)

 indiv.plot <- indv$plot

 indiv.plot
#now in a loci specific way
loci <- miss_curves_locus(mysis)
loci.plot <- loci$plot+
  xlim(0,23020)+
  ylim(.5,1)

loci.plot
#Pulling out the data
#So, in theory, if we look at this and decide that we want to keep all our individuals since no one exceeds 60% missingness
#individuals and roughly 18000 positions, we should be able to do so in such a way 
#that we have no individuals with more than about 60% missing data, and no loci that should have more than 30% missing data.
#this looks like it will optimize the number of individuals which is more important for our demographic based 
#questions 

#Let's see:

dir(pattern="C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/pre_filter/cleaned_indv")

clean <- miss_curves_indv(mysis, clean_pos = 18000 , clean_indv = 168)  
#> Picking out clean_pos and clean_indv and writing them to 
#We can make a picture of the result

clean$plot

#we can read the "clean" file back in
mysis_clean <- scan_012("./cleaned_indv168_pos18000", gz=FALSE, gzpos = FALSE, posfile_columns = 2)

#and then look at the distribution of missing data to make sure it matches what the plots estimated
mysis_clean_miss <- mysis_clean == -1
dim(mysis_clean)
dim(mysis_clean_miss)
missing_perc_in_indiv <- rowSums(mysis_clean_miss) / ncol(mysis_clean_miss)
missing_perc_in_loci <- colSums(mysis_clean_miss) / nrow(mysis_clean_miss)

par(mfrow=c(1,2))
hist(missing_perc_in_indiv, main =  "Individuals") #these would be able to show you if you need to drop some individulas from above mysis_clean, there is one idividual with less than 35%, otherwise it looks like most individuals are 42% to more than 50% missing so those should be filtered out

hist(missing_perc_in_loci, main= "Positions") ##looking at hist to see how you can adjust individuals, but it looks like there is a heavy right skew which could be due to alinment errors?

#calculate heterozysoity per individual
 
mysis_hets <- mysis_clean == 1
mysis_per_in_loci <- colSums(mysis_hets) / nrow(mysis_hets)
mysis_per_in_indiv <- rowSums(mysis_hets) / ncol(mysis_hets)


par(mfrow=c(1,2))
hist(mysis_per_in_indiv, main =  "Individuals") #individuals look normally distributed for heterozygosity
hist(mysis_per_in_loci, main= "Positions") #looks like there is a long tail in SNPs which is due to slightly relaxed filter in individuals. This will be mitigated with further filtering for one random snp 
  

#Can check to see which insidivuals and snps are dropped in the filtering 
het_remove_het <- mysis_clean[,mysis_per_in_loci < 0.5]
het_remove_het <- mysis_clean[, mysis_per_in_indiv < 0.5] #individual dist of missingness is fairly normal, so this isn't critical

dim(het_remove_het)

#option to write it as a table 
#write.table(missing_data,file=het_remove_het)

#figure out which individuals are dropped in filtering 
unclean <- as_tibble(rownames(mysis))

dim(unclean)

clean <- as_tibble(rownames(het_remove_het))

#option to see who would be removed 
removed_indiv <- anti_join(unclean, clean, by="value")


```


Use PLINK for filtering based on the visual assessment from GenoscapeRTools 

**Description**: Plink is an open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. Based on results from GenoscaprRTools, will filter for missingness and quality for final SNP matrix. 

**Associated Data**: plink format files from populations. Output: final filtered SNP dataset in structure, plink, and vcf format

working in the random_snp directory

```{sh}

#Rerun popualitons to output a plink raw file and then create a subest of one random snp per locus to avoid bias associated with physical linkage (O'Leary et al., 2018),

#!/bin/bash

#SBATCH --job-name=populations
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/random_snp/populations.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/random_snp/populations.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com
#SBATCH --mail-type=END


OUT=/home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/random_snp
POPMAP=/home/ghalambor/scratch_dir/shrimp/raw_data/mysis_popmap_pheno
FILEDIR=/home/ghalambor/scratch_dir/shrimp/denovo_map_pheno

populations -P $FILEDIR -O $OUT -M $POPMAP -p 1 -R 0.3 --min-mac 2 --plink --write-random-snp

#then navigate to the random snp folder and run the following code chunks in plink 

#Populations ouput prior to filtering
#Kept 61171 loci, composed of 17902947 sites; 184464 of those sites were filtered, 52169 variant sites remained

# First, we filter individuals that have greater than 60% missing data

--mind [maximum per-sample]

#second, we filter SNPs with missing call rates exceeding the provided value. In this case more than 20%
--geno [maximum per-variant]

#Third, filter SNPs that are out of Hardy-Weinberg equilibrium P-value like chavira pizarro 2019, but PLINK recommends using a very small p-value as serious genotyping errors often yield extreme p-values (like 1e-50) which are detected by any reasonable configuration of this test
 


# finally, we filter loci We used MAC =3 because it requires the markers to be genotyped in at least 1 heterozygote and 1 homozygote, or three heterozygous individuals, to keep a SNP. This retains the maximum amount of variation
#From the Radiator Manual 
#Using count or frequency to remove a SNPs ? The preferred choice in radiator as changed from frequency to count, because we think the filtering should not alter the spectrum and this is only achieved if the same criteria is applied for each SNP.

#Even small differences in missing data between RADseq markers generates differences in MAF frequency thresholds applied.

#Example with a datset consisting of N = 36 individuals and 3 SNPs with varying level of missing genotypes:

#SNP number : number samples genotypes : REF/ALT counts

#SNP1 : 36 : 69/3

#SNP2 : 30 : 65/3

#SNP3 : 24 : 45/3

#Each SNPs have the same alternate allele count, corresponding to 2 individuals with the polymorphism: 1 homozygote + 1 heterozygote. Applying a MAF threshold of 0.05 would mean that SNP3 would be blacklisted (24 * 2 * 0.05 = 2.4 alt alleles required to pass).

#Using count instead of frequency allows each RADseq markers, with varying missing data, to be treated equally


#all the filters in a single line 

plink --file populations.plink --mind .60 --geno .50  --mac 3 --hwe 1e-50 --recode --out ./missing_test_for_neut_structure/mysis_clean --allow-extra-chr

#Performing single-pass .bed write (48670 variants, 168 people).
#4785 variants removed due to minor allele threshold(s)
#17012 variants and 168 people pass filters and QC.

plink --file populations.plink --mind .50 --geno .50  --mac 3 --hwe 1e-50 --recode --out ./missj_test_clean --allow-extra-chr


#recode for structure format. 
plink --file mysis_clean --recode structure --out mysis_clean --allow-extra-chr

#recode for raw plink format 
plink --file mysis_clean --recode AD --out mysis_clean --allow-extra-chr

#recode for vcf format 
plink --file mysis_clean --recode vcf --out mysis_clean --allow-extra-chr

#recode for a bianary bed file
#additional files (such as .bed files) can then be produced in PLINK

plink --file mysis_clean --make-bed --out mysis_clean --allow-extra-chr


#run a quick check to see how much relatedness there is between individuals bu using the rlatedness tool in VCFTools 
#need to activate a conda environment first 

--relatedness

#This option is used to calculate and output a relatedness statistic based on the method of Yang et al, Nature Genetics 2010 (doi:10.1038/ng.608). Specifically, calculate the unadjusted Ajk statistic. Expectation of Ajk is zero for individuals within a populations, and one for an individual with themselves. The output file has the suffix ".relatedness".


conda activate

vcftools --vcf mysis_clean.vcf --relatedness



```


Plot out the relatedness phi to see if there are any closely related individuals
```{r}

library(ggplot2)
library(tidyverse)
library(ggthemes)


relatedness <- as_tibble(read.delim( "C:/Users/Rebecca/Colostate/GhalamborLab - mysis/log files/out.relatedness"))

ggplot(relatedness, aes(x=INDV1, y=RELATEDNESS_AJK))+ 
  geom_point()+
  theme_few()


#it doesn't seem like any of the shrimp are closely related. The highest one is .10 in Dillon, the ones that are around 1 are all pairwise comparisons of the same individual. 
```

We also want to have a separate dataset specifically for calculating neutral population structure statistics. Following the recommendations of Schmidt et al. 2020, who used no MAC cutoff (--min-mac 1) and estimated heterozygosity across every site rather than every polymorphic site. In the output from the Stacks.v2 program “Populations”, this corresponds to the entries in the “# All positions (variant and fixed)” subsection. Schmidt also advocated for a 0% missingness threshold which is too much for us since there are only 41 variants with no missing data. A little crazy. 
Therefore, we will test a few different missingness thresholds for loci (0%, 10%, 20%) without using any preliminary filtering in populations (MAC in particular). That way we will retain all positions (including singletons) which have no or very limited missingness to increase confidence in our measures of heterozygosity and pi while also including our "good" SNPs that passed missing QC. Purely for neutral popstats. Wherease we can use our imputed dataset for neutral popualiton genetics

So this dataset will specifically be used for estimates of pi, observed and expected heterozygosity. Whereas the more librearal filter dataset will be used for Ne and 


```{sh}


#!/bin/bash

#SBATCH --job-name=populations
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_stats/populations.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_stats/populations.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com
#SBATCH --mail-type=END


OUT=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_stats
POPMAP=/home/ghalambor/scratch_dir/shrimp/raw_data/mysis_popmap_pheno
FILEDIR=/home/ghalambor/scratch_dir/shrimp/denovo_map

populations -P $FILEDIR -O $OUT -M $POPMAP --min-mac 1 --plink

#then navigate to the neutral snp folder and run the following code chunks in plink 


#all the filters in a single line 

plink --file populations.plink --geno 0 --recode --out mysis_clean_0 --allow-extra-chr

#Total genotyping rate is 0.205002.
#943197 variants removed due to missing genotype data (--geno).
#41 variants and 168 people pass filters and QC.


plink --file populations.plink --geno .10 --recode --out mysis_clean_10 --allow-extra-chr
#Total genotyping rate is 0.205002.
#3036 variants and 168 people pass filters and QC.

plink --file populations.plink --geno .20 --recode --out mysis_clean_20 --allow-extra-chr
#Total genotyping rate is 0.205002.
#12208 variants and 168 people pass filters and QC.


#recode for structure format. 
plink --file mysis_clean --recode structure --out mysis_clean --allow-extra-chr

#recode for raw plink format 
plink --file mysis_clean --recode AD --out mysis_clean --allow-extra-chr

#recode for vcf format 
plink --file mysis_clean --recode vcf --out mysis_clean_0_for_neutral_popstats --allow-extra-chr

#recode for a bianary bed file
#additional files (such as .bed files) can then be produced in PLINK

plink --file mysis_clean --make-bed --out mysis_clean --allow-extra-chr


mysis_clean_for_neutral_popstats.vcf

```


Get list of final filtered dataset of individuals
However, none of our individuals exceeded 60% missingness so this part is mostly for reference. 

```{r}

library(vcfR)
#read in the colnames of the vcf 
filtered <- as_tibble(read.table("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/filtered_data/mysis_clean.raw"), sep=" ")

filtered <- filtered[-1,2]

filtered <- filtered %>% 
  dplyr::rename("INDIVIDUALS"="V2")

#use antijoin of the popualtions to determine who was filtered out 

all <- as_tibble(read.delim("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/strata.mysis_pheno"), sep=" ")

all <- all[,1]

dropped_indiv <- anti_join(all, filtered, by="INDIVIDUALS")


#write.csv(dropped_indiv, "C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/filtered_data/filtered_individuals.csv", row.names = F)

```


**Impute missing data using Linkimpute**
Because RDA requires complete data frames (see section 2; Identification of population Structure Associated with Habitat), we will have to impute the remaining missing genotypes. Even after filtering, we have 36% missing genotype data accross all our data, and according to subsequent popualtion genetics analyses we have a single anscetral population of K=1 (see Popualiton Genetics results), so we won't be able to use lfmm to impute the remaining missing data. Therefore, we will use linkimpute software (http://www.cultivatingdiversity.org/software.html), which implements the LD-kNNi algorithm described in the following publication: 
Money D, Gardner KM, Migicovsky Z, Schwaninger H, Zhong GY, Myles S (2015) LinkImpute – fast and accurate genotype imputation for non-model organisms. G3: Genes, Genomes, Genetics 5:2383 [PubMed:26377960]
The benefit of LinkImpute is that it is designed for non-model organisms without a reference genome and can be easily implemented with plink files. We wil use this imputed data for all subsequent analyses.  

```{sh eval=FALSE, include=FALSE}

#intalled LinkImpute by dowloading from the source website
wget http://www.cultivatingdiversity.org/uploads/5/8/2/9/58294859/20200114_linkimpute.tar.gz


#extract the files
tar -xvzf 20200114_linkimpute.tar.gz

#be sure to activate a conda window so that java in enabled 

conda activate 

# This is the file path for linkimpute 
/home/ghalambor/scratch_dir/programs/LinkImpute.jar 

#All options other than INFILE and OUTFILE are optional java -jar LinkImpute.jar [-p | -q | -a | -v] INFILE OUTFILEOptions are:-p Use plink raw file #format (default) -q Use plink ped file format -a Use array file format and -v for vcf

java -jar /home/ghalambor/scratch_dir/programs/LinkImpute.jar -q ./mysis_clean.ped ./imputed_data_Linkimpute/mysis_clean_imputed.ped

#can then take the imputed .ped file and use the .map file from the non imputed data to convert to needed formats 
#need to change hte .map filename to mysis_clean_imputed


plink --file mysis_clean_imputed --recode AD --out mysis_clean_imputed --allow-extra-chr

plink --file mysis_clean_imputed --recode vcf --out mysis_clean_imputed --allow-extra-chr

plink --file mysis_clean_imputed --make-bed --out mysis_clean_imputed --allow-extra-chr



```




**Filter to just the neutral SNPs by removing outliers flagged by PCAdapt** 
	**Description**: PCAdapt uses Principal Components Analysis (PCA) to identify loci showing strong signatures of selection relative to neutral background genomic variation. Identify putatively adaptive outlier loci using a false discovery rate of 10% and filter these outliers out for downstream landscape genomic analyses to avoid confounding neutral demographic patterns with patterns generated by loci under selection.
**Associated Data**: convert vcf file from populaitons to PLINK bed file since converter vcf to pcadapt is deprecated in PCAdapt. 
		**Results**: list of outlier loci that can be removed from the whitelist produced by radiator to create a secondary "neutral loci" dataset by re-running populations with neutral whitelist


```{r message=FALSE, warning=FALSE}
library(pcadapt)
library(qvalue)
library(tidyverse)
library(pegas)
library(caroline)
library(tidyverse)


file<-"C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/filtered_data/imputed_data/mysis_clean_imputed.ped"


mysis.pcadapt <- read.pcadapt(file,type="ped")


#Choose the appropriate K value from scree plot, run PCAdapt, and get summary.
#it looks like K=1 is the most supported (see results) so we will use that criterion for detecting loci under selection

x <- pcadapt(mysis.pcadapt,K=1)

summary(x)


#A Manhattan plot displays log10 of the p-values.

plot(x,option="manhattan")

#Check the distribution of the p-values using a Q plot, showing the top 5% lowest p-values.

plot(x,option="qqplot",threshold=0.05)

#Use q histogram of p-values to confirm that most of the p-values follow the uniform distribution,
#and that there is an excess of small p-values indicating the presence of outliers.

hist(x$pvalues,xlab="p-values",main=NULL,breaks=50)

#For a given ± (number between 0 and 1), SNPs with q-values less than alpha will be considered outliers with an expected false discovery rate bounded by Î±. 
#The false discovery rate is defined as the percentage of false discoveries expected among the list of outlier candidate SNPs.


qval <- qvalue(x$pvalues)$qvalues

#provide a list of candidate SNPs with an expected false discovery rate less than 10%.
alpha <- 0.10
outliers <- which(qval<alpha)
outliers <- as_tibble(outliers, quote=F)

#Looks like we have 69 potential outliers

# Filter these out so we have a final clean dataset. Maybe there is some effect of demographic processes at play 
#note that PCAdapt outputs the "order" of the loci rather than the IDs of the loci themselves

#to exclude the outliers we need to generate a blacklist that can be read by popualitons so we can create a "neutral" list of SNPs for popualtion genetics analyses: 

#read in the "clean" vcf of the snps that passed QC in Plink. The collumns are the snp name iwth LocusID_Col which can be used to generate a whitelist of snps for populations in Stacks


gen <-  read.vcf("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/filtered_data/imputed_data/mysis_clean_imputed.vcf", to=17012)

as_tibble(gen)

#create a sequence from 1 to limit of the vcf so the collumns has numbers that can be filtered out using the list from pcadapt
col_id <- as_tibble(seq(1,17012,1))


col_id$snp =  colnames(gen)

#now we can join to see which are the outlier snps
blacklist <- left_join(outliers,col_id, by="value")

##make a new collumn with the snp name sepparated since its just the locus and col information

blacklist<- separate(blacklist,snp ,
               c("Locus ID", "Col"))

#Extract the colnames from the gen file which are our snps that passed QC in Plink. Since that has the LocusId_Col information we can use it to make a final white list of "neutral" snps by filtering out the outliers 

col_id<- separate(col_id, snp ,
               c("Locus ID", "Col"))

  
#make sure the classes match with the balcklist
col_id$`Locus ID` <- as.character(col_id$`Locus ID`)
col_id$Col <- as.character(col_id$Col)


#filter out the blacklist of outliers flagged by PCAdapt

whitelist <- anti_join(col_id, blacklist, by=c("Locus ID", "Col"))

#get rid of the value collumn
whitelist <- whitelist %>% 
  dplyr::select(c("Locus ID", "Col"))

#then write the tab delinated text file that is our new whitelist of "neutral" snps
#write.delim(whitelist, file = "C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/filtered_data/imputed_data/neutral_snps/neutral_whitelist.txt", quote = FALSE, row.names = FALSE, sep = "\t",col.names = FALSE )

#fyi need to get rid of those linebreaks again
# tr -d '\r' < neutral_whitelist.txt > neutral_whitelist

```


Can now use these SNPs that passed using populations as well as a popmap of filtered individuals that didn't pass QC.
We will also get our measures of neutral population structure including Fst, 
modifying the 02.6-populations_pi.sh


```{sh}

#!/bin/bash

#SBATCH --job-name=pop_neut
#SBATCH --output=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_snps/populations_pi.out
#SBATCH --error=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_snps/populations_pi.err
#SBATCH --mail-user=Rebecca.G.Cheek@gmail.com
#SBATCH --mail-type=END


OUT=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_snps/neutral_stats
POPMAP=/home/ghalambor/scratch_dir/shrimp/raw_data
FILEDIR=/home/ghalambor/scratch_dir/shrimp/denovo_map/
WHITE=/home/ghalambor/scratch_dir/shrimp/denovo_map/neutral_snps

populations -P $FILEDIR -W $WHITE/neutral_whitelist -O $OUT -M $POPMAP/mysis_popmap_pheno


```


Rename the output files and impute the neutral dataset and convert to different formats for section 2 (Neutral PopGen)

```{sh}
#note, might have to delete header in the ped file from stacks 

java -jar /home/ghalambor/scratch_dir/programs/LinkImpute.jar -q ./mysis_clean_neutral.ped ./imputed_data/mysis_clean_imputed_neutral.ped

#can then take the imputed .ped file and use the .map file from the non imputed data to convert to needed formats 
#need to change the .map filename to mysis_clean_imputed

plink --file mysis_clean_imputed_neutral --recode AD --out mysis_clean_imputed_neutral --allow-extra-chr

plink --file mysis_clean_imputed_neutral --recode vcf --out mysis_clean_imputed_neutral --allow-extra-chr

plink --file mysis_clean_imputed_neutral --make-bed --out mysis_clean_imputed_neutral --allow-extra-chr


```


We have 17,012 whitelisted markers. Following populations, 16,943 variant SNPs remained for tests of neutral population structure.



lets also get an estimate of LD decay using all 17,012 SNPs


# Use this script and adjust ld window to see how correlated snps are within 100 kb 
#check LD using the imputed dataset from LinkImpute

```{sh}
#be sure to activate a conda window to load vcftools
vcftools --vcf ./mysis_clean_imputed.vcf --ld-window-bp 100000 --max-alleles 2 --min-alleles 2 --geno-r2 --out ./link100


#and with 500 kb
vcftools --vcf ./mysis_clean_imputed.vcf --ld-window-bp 500000 --max-alleles 2 --min-alleles 2 --geno-r2 --out ./link500

#also try it with the unfiltered data to see how it differs? 

vcftools --vcf ./ISSJ.ZF.ordered_imputed_BEAGLE.vcf --ld-window-bp 100000 --max-alleles 2 --min-alleles 2 --geno-r2 --out ./link100




```


Then plot out the LD decay using a custom function 

```{r}
#plot the LD with R2 correlations on y and bp on x
library(tidyverse)

plotDecayLD <- function(dfr, chr, xlim=c(NA,NA),ylim=c(NA,NA),avgwin=0,minr2) {
  if(missing(dfr)) stop("Input data.frame 'dfr' missing.")
  
  if(!missing(chr)) {
    ld <- filter(ld,CHROM_A==get("chr") & CHROM_B==get("chr"))
  }
  
  if(!missing(minr2)) {
    ld <- filter(ld,ld$R2>get("minr2"))
  }
  
  ld <- ld %>% arrange(R2)
  
  ld$dist <- ld$DISTANCE
  
  if(avgwin>0) {
    ld$distc <- cut(ld$dist, breaks=seq(from=min(ld$dist),to=max(ld$dist),by=avgwin))
    ld <- ld %>% group_by(distc) %>% summarise(dist=mean(dist),R2=mean(R2))
  }
  
  ggplot(ld,aes(x=dist,y=R2))+
    geom_point(shape=20,size=0.15,alpha=0.7)+
    geom_smooth()+
    scale_x_continuous(limits=xlim)+
    scale_y_continuous(limits=ylim)+
    labs(x="Distance (Bases)",y=expression(LD~(r^{2})))+
    theme_bw(base_size=14)+
    theme(panel.border=element_blank(),
          axis.ticks=element_blank())  
    geom_vline(xintercept=25000, color="red", linetype="dashed", size=1)  %>%

    return()
}


#filtered data 

ld <- read.delim("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/link100.geno.ld")

#alternatively with the unfiltered data
ld <- read.delim("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/pre_filter/link500.geno.ld")

ld$DISTANCE <- ld$POS2-ld$POS1


#Don't have chromosomal information so can just use position by setting chr to unk

ld<- ld %>% 
  rename("R2"="R.2",
         "CHROM_A"="POS1",
         "CHROM_B"="POS2")

ld$CHROM_A <- "un"



ld$CHROM_B <- ld$CHROM_A

ld <- ld %>% 
  dplyr::select(c("R2","CHROM_A", "CHROM_B", "DISTANCE" ))


plotDecayLD(ld , minr2=0.25 )




```

Overall LD is pretty low. 

We can also get a sense of read depth to see if ploidy is playing a roll. Especially for the Jumbo shrimp.
Need to use this script on the non-clone filtered data

```{sh}
#!/bin/bash

# set path to vcf file for all analyses
VCFALL=/home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/populations.snps.vcf


## mean depth per individual
vcftools --vcf $VCFALL --depth -c > /home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/calculate_coverage_and_reads/depth_summary.txt

## mean depth per site
vcftools --vcf $VCFALL --site-depth -c > /home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/calculate_coverage_and_reads/depth_summary_site.txt

## missingness per individual
vcftools --vcf $VCFALL --missing-indv --out /home/ghalambor/scratch_dir/shrimp/denovo_map_pheno/pre-filter/calculate_coverage_and_reads/missingind


##LD estimates

vcftools --vcf $VCFALL --ld-window-bp 100000 --max-alleles 2 --min-alleles 2 --geno-r2 --out ./link100

## read tables into R to calculate stats


```



```{r}

mdepth <- read.delim("C:/Users/Rebecca/Colostate/GhalamborLab - mysis/data/pre_filter/depth_summary.txt")


```



